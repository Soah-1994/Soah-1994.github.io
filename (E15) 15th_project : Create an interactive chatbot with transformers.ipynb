{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "critical-behavior",
   "metadata": {},
   "source": [
    "## 1. 패키지 로딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pretty-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-watch",
   "metadata": {},
   "source": [
    "## 2. 포지셔널 행렬 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "limiting-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(position = tf.range(position, dtype = tf.float32)[:, tf.newaxis],\n",
    "                                     i        = tf.range(d_model,  dtype = tf.float32)[tf.newaxis, :],\n",
    "                                     d_model  = d_model)\n",
    "        # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "        sines   = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis = -1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "departmental-research",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 포지셔널 행렬 시각화하여 테스트해보기\n",
    "sample_pos_encoding = PositionalEncoding(50, 512)\n",
    "\n",
    "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-expansion",
   "metadata": {},
   "source": [
    "## 3. 스케일드 닷 프로덕트 어텐션 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "casual-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"어텐션 가중치를 계산한다.\"\"\"\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b = True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    depth  = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # add the mask to zero out padding tokens\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(logits, axis = -1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-typing",
   "metadata": {},
   "source": [
    "## 4. 멀티 헤드 어텐션 함수 만들기\n",
    "#### 내부적으로 스케일드 닷 프로덕트 어텐션 함수를 호출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "shaped-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, name = \"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name = name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model   = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units = d_model)\n",
    "        self.key_dense   = tf.keras.layers.Dense(units = d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units = d_model)\n",
    "        self.dense       = tf.keras.layers.Dense(units = d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(inputs, shape = (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm = [0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs[\"query\"], inputs[\"key\"], inputs[\"value\"], inputs[\"mask\"]\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # linear layers\n",
    "        query = self.query_dense(query)\n",
    "        key   = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 병렬 연산을 위한 머리를 여러 개 만든다.\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key   = self.split_heads(key,   batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 스케일드 닷 프로덕트 어텐션 함수\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm = [0, 2, 1, 3])\n",
    "\n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)한다.\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        # final linear layer\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-zimbabwe",
   "metadata": {},
   "source": [
    "## 5. 패딩 마스크(Padding Masking) 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "light-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 마스크 함수\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fifth-valuable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 1. 0. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 1. 0. 0.]]]], shape=(2, 1, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 잘 작동되는지 테스트 하기\n",
    "print(create_padding_mask(tf.constant([[1, 2, 0, 3, 0], [0, 0, 0, 4, 5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-server",
   "metadata": {},
   "source": [
    "## 6. 룩 어헤드 마스킹(Look-ahead masking, 다음 단어 가리기) 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "undefined-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 룩 어헤드 마스킹 함수\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len         = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask    = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "designed-christianity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[1. 1. 1. 1. 1.]\n",
      "   [1. 0. 1. 1. 1.]\n",
      "   [1. 0. 0. 1. 1.]\n",
      "   [1. 0. 0. 0. 1.]\n",
      "   [1. 0. 0. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 잘 작동되는지 테스트 하기\n",
    "print(create_look_ahead_mask(tf.constant([[1, 2, 3, 4, 5]])))\n",
    "\n",
    "# 룩 어헤드 마스킹과 패딩 마스킹은 별개이므로, 룩 어헤드 마스킹을 수행할 때 만약 숫자 0이 있다면 0도 패딩 해야 한다.\n",
    "# 그래서 룩 어헤드 마스킹 함수에는 내부적으로 패딩 마스크 함수도 호출하고 있다.\n",
    "print(create_look_ahead_mask(tf.constant([[0, 5, 1, 5, 5]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ancient-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 개의 서브 층을 가지는 하나의 인코더 층을 구현하는 함수 만들기\n",
    "# 인코더 하나의 레이어를 함수로 구현한다.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재한다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name = \"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape = (None, d_model),  name = \"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape = (1, 1, None), name = \"padding_mask\")\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention = MultiHeadAttention(d_model, num_heads, name = \"attention\")({\n",
    "        \"query\": inputs,\n",
    "        \"key\"  : inputs,\n",
    "        \"value\": inputs,\n",
    "        \"mask\" : padding_mask\n",
    "    })\n",
    "\n",
    "    # 어텐션의 결과는 Dropout과 Layer Normalization 이라는 훈련을 돕는 테크닉을 수행한다.\n",
    "    attention = tf.keras.layers.Dropout(rate = dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(inputs + attention)\n",
    "\n",
    "    # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units = units, activation = \"relu\")(attention)\n",
    "    outputs = tf.keras.layers.Dense(units = d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행한다.\n",
    "    outputs = tf.keras.layers.Dropout(rate = dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs = [inputs, padding_mask], outputs = outputs, name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "honey-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 층을 쌓아서 인코더 만들기\n",
    "def encoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name = \"encoder\"):\n",
    "    inputs = tf.keras.Input(shape = (None,), name = \"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape = (1, 1, None), name = \"padding_mask\")\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate = dropout)(embeddings)\n",
    "\n",
    "    # num_layers 만큼 쌓아올린 인코더의 층.\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(units     = units,\n",
    "                                d_model   = d_model,\n",
    "                                num_heads = num_heads,\n",
    "                                dropout   = dropout,\n",
    "                                name      = \"encoder_layer_{}\".format(i),\n",
    "                               )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(inputs = [inputs, padding_mask], outputs = outputs, name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-identifier",
   "metadata": {},
   "source": [
    "## 8. 디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "based-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세 개의 서브 층을 가지는 하나의 디코더 층을 구현한 함수 만들기\n",
    "# 디코더 하나의 레이어를 함수로 구현\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재한다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name = \"decoder_layer\"):\n",
    "    inputs          = tf.keras.Input(shape = (None, d_model), name = \"inputs\")\n",
    "    enc_outputs     = tf.keras.Input(shape = (None, d_model), name = \"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape = (1, None, None), name = \"look_ahead_mask\")\n",
    "    padding_mask    = tf.keras.Input(shape = (1, 1, None),    name = \"padding_mask\")\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(d_model, num_heads, name = \"attention_1\")(inputs = {\n",
    "        \"query\": inputs,\n",
    "        \"key\"  : inputs,\n",
    "        \"value\": inputs,\n",
    "        \"mask\" : look_ahead_mask\n",
    "    })\n",
    "\n",
    "    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행한다.\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(attention1 + inputs)\n",
    "\n",
    "    # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(d_model, num_heads, name = \"attention_2\")(inputs = {\n",
    "        \"query\": attention1,\n",
    "        \"key\"  : enc_outputs,\n",
    "        \"value\": enc_outputs,\n",
    "        \"mask\" : padding_mask\n",
    "    })\n",
    "\n",
    "    # 마스크드 멀티 헤드 어텐션의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행한다.\n",
    "    attention2 = tf.keras.layers.Dropout(rate = dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(attention2 + attention1)\n",
    "\n",
    "    # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units = units, activation = \"relu\")(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units = d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행한다.\n",
    "    outputs = tf.keras.layers.Dropout(rate = dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(inputs = [inputs, enc_outputs, look_ahead_mask, padding_mask], outputs = outputs, name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "anonymous-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 층을 쌓아 디코더 만들기\n",
    "def decoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name = \"decoder\"):\n",
    "    inputs          = tf.keras.Input(shape = (None,),         name = \"inputs\")\n",
    "    enc_outputs     = tf.keras.Input(shape = (None, d_model), name = \"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape = (1, None, None), name = \"look_ahead_mask\")\n",
    "\n",
    "    # 패딩 마스크\n",
    "    padding_mask = tf.keras.Input(shape = (1, 1, None), name = \"padding_mask\")\n",
    "    \n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # Dropout이라는 훈련을 돕는 테크닉을 수행한다.\n",
    "    outputs = tf.keras.layers.Dropout(rate = dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(units     = units,\n",
    "                                d_model   = d_model,\n",
    "                                num_heads = num_heads,\n",
    "                                dropout   = dropout,\n",
    "                                name      = \"decoder_layer_{}\".format(i),\n",
    "                               )(inputs = [outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(inputs = [inputs, enc_outputs, look_ahead_mask, padding_mask], outputs = outputs, name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-switzerland",
   "metadata": {},
   "source": [
    "## 9. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mexican-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수 만들기\n",
    "# 여기서는 정규 표현식을 이용하여 구두점을 제거하여 단어를 토크나이징할 때 방해가 되지 않도록 정제하는 것이 목표이다.\n",
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    # 단어와 구두점(punctuatuin) 사이의 거리를 만든다.\n",
    "    # 예를 들어 \"나는 학생입니다.\" -> \"나는 학생입니다 .\" 같이\n",
    "    # student와 온점 사이에 거리를 만든다.\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    # (a-z, A-Z, 가-힣, 숫자, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 \" \"로 대체한다.\n",
    "    sentence = re.sub(r\"[^a-zA-Z가-힣\\d?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "infectious-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 로드하는 동시에 전처리 함수를 사용하여 질문과 답변의 쌍으로 전처리한다.\n",
    "def load_conversations():\n",
    "    with open(path_to_dataset, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    id2line = []\n",
    "    for line in lines:\n",
    "        parts  = line.replace(\"\\n\", \"\").split(\",\")\n",
    "        id2line.append(parts)\n",
    "    \n",
    "    inputs, outputs = [], []\n",
    "    for i in range(len(id2line) - 1):\n",
    "        inputs.append(preprocess_sentence(id2line[1:][i][0]))\n",
    "        outputs.append(preprocess_sentence(id2line[1:][i][1]))\n",
    "    \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "recent-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = tf.keras.utils.get_file(fname  = \"ChatbotData.csv\",\n",
    "                                          origin = \"https://github.com/songys/Chatbot_data/raw/master/ChatbotData%20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "communist-checkout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11823\n",
      "전체 샘플 수 : 11823\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 로드하고 전처리하여 질문을 questions, 답변을 answers에 저장합니다.\n",
    "questions, answers = load_conversations()\n",
    "print(\"전체 샘플 수 :\", len(questions))\n",
    "print(\"전체 샘플 수 :\", len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-discharge",
   "metadata": {},
   "source": [
    "## 10. 병렬 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-raising",
   "metadata": {},
   "source": [
    "### 10_1. 단어장(Vocabulary) 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "irish-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size = 2 ** 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "objective-anniversary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8139]\n",
      "END_TOKEN의 번호 : [8140]\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 문장 생성 과정에서 사용할 시작 토큰과 종료 토큰을 단어장에 추가하여서 정수를 부여해 준다.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# 토큰이 잘 생성됬는지 테스트 하기\n",
    "print(\"START_TOKEN의 번호 :\", [tokenizer.vocab_size])\n",
    "print(\"END_TOKEN의 번호 :\",   [tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "guilty-leone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8141\n"
     ]
    }
   ],
   "source": [
    "# 생성한 토큰을 고려하여 단어장 크기 설정하기\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-sense",
   "metadata": {},
   "source": [
    "### 10_2. 각 단어를 고유한 정수로 인코딩(Integer encoding) 및 패딩(Padding) 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "instructional-sherman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 1번째 질문 샘플: [7879, 4179, 3036, 40]\n",
      "정수 인코딩 후의 1번째 답변 샘플: [3816, 75, 7858, 1]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 1번째 샘플에 대해서 정수 인코딩 작업을 수행한다.\n",
    "# 각 토큰을 고유한 정수로 변환한다.\n",
    "print(\"정수 인코딩 후의 1번째 질문 샘플: {}\".format(tokenizer.encode(questions[0])))\n",
    "print(\"정수 인코딩 후의 1번째 답변 샘플: {}\".format(tokenizer.encode(answers[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "associate-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩할 때 필요한 문장 최대 길이 설정하기\n",
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "restricted-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩과 패딩을 하는 함수 만들기\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가한다.\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "        # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "\n",
    "    # 최대 길이 40으로 모든 데이터셋을 패딩한다.\n",
    "    tokenized_inputs  = tf.keras.preprocessing.sequence.pad_sequences(tokenized_inputs,  maxlen = MAX_LENGTH, padding = \"post\")\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_outputs, maxlen = MAX_LENGTH, padding = \"post\")\n",
    "\n",
    "    return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "operating-patio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8141\n",
      "필터링 후의 질문 샘플 개수 : 11823\n",
      "필터링 후의 답변 샘플 개수 : 11823\n"
     ]
    }
   ],
   "source": [
    "# 만든 함수를 이용하여 인코딩과 패딩하기\n",
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "\n",
    "print(\"단어장의 크기 :\", (VOCAB_SIZE))\n",
    "print(\"필터링 후의 질문 샘플 개수 : {}\".format(len(questions)))\n",
    "print(\"필터링 후의 답변 샘플 개수 : {}\".format(len(answers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-motivation",
   "metadata": {},
   "source": [
    "## 10_3. 교사 강요(Teacher Forcing) 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "attached-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE  = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용한다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거한다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({\"inputs\" : questions, \"dec_inputs\" : answers[:, :-1]}, {\"outputs\" : answers[:, 1:]},))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-struggle",
   "metadata": {},
   "source": [
    "## 11. 모델 정의 및 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "revised-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞서서 만든 인코더 층 함수와 디코더 층 함수를 사용하여 트랜스포머 함수를 정의한다.\n",
    "def transformer(vocab_size, num_layers, units, d_model, num_heads, dropout, name = \"transformer\"):\n",
    "    inputs     = tf.keras.Input(shape = (None,), name = \"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape = (None,), name = \"dec_inputs\")\n",
    "\n",
    "    # 인코더에서 패딩을 위한 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(create_padding_mask, output_shape = (1, 1, None), name = \"enc_padding_mask\")(inputs)\n",
    "\n",
    "    # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용한다.\n",
    "    # 내부적으로 패딩 마스크도 포함되어 있다.\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(create_look_ahead_mask, output_shape = (1, None, None), name = \"look_ahead_mask\")(dec_inputs)\n",
    "\n",
    "    # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹한다.\n",
    "    # 디코더에서 패딩을 위한 마스크\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(create_padding_mask, output_shape = (1, 1, None), name = \"dec_padding_mask\")(inputs)\n",
    "\n",
    "    # 인코더\n",
    "    enc_outputs = encoder(vocab_size = vocab_size, \n",
    "                          num_layers = num_layers, \n",
    "                          units      = units, \n",
    "                          d_model    = d_model, \n",
    "                          num_heads  = num_heads,\n",
    "                          dropout    = dropout,)(inputs = [inputs, enc_padding_mask])\n",
    "\n",
    "    # 디코더\n",
    "    dec_outputs = decoder(vocab_size = vocab_size,\n",
    "                          num_layers = num_layers,\n",
    "                          units      = units,\n",
    "                          d_model    = d_model,\n",
    "                          num_heads  = num_heads,\n",
    "                          dropout    = dropout,)(inputs = [dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units = vocab_size, name = \"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs = [inputs, dec_inputs], outputs = outputs, name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-consciousness",
   "metadata": {},
   "source": [
    "### 11_1. 모델 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "revolutionary-strength",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 (None, None, 512)    13636096    inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, None, 512)    19945984    dec_inputs[0][0]                 \n",
      "                                                                 encoder[1][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8141)   4176333     decoder[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 37,758,413\n",
      "Trainable params: 37,758,413\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session() # 최근의 만든 모델이 있다면 지워준다.\n",
    "\n",
    "# 하이퍼 파라미터\n",
    "NUM_LAYERS = 6                   # 인코더와 디코더의 층의 개수\n",
    "D_MODEL    = 512                 # 인코더와 디코더 내부의 입력, 출력의 고정 차원\n",
    "NUM_HEADS  = 8                   # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS      = 512                 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT    = 0.1                 # dropout의 비율\n",
    "\n",
    "model = transformer(vocab_size = VOCAB_SIZE,\n",
    "                    num_layers = NUM_LAYERS,\n",
    "                    units      = UNITS,\n",
    "                    d_model    = D_MODEL,\n",
    "                    num_heads  = NUM_HEADS,\n",
    "                    dropout    = DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-billy",
   "metadata": {},
   "source": [
    "### 11_2. 손실 함수(Loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "arctic-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape = (-1, MAX_LENGTH - 1))\n",
    "    loss   = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = \"none\")(y_true, y_pred)\n",
    "    mask   = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss   = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-carry",
   "metadata": {},
   "source": [
    "### 11_3. 커스텀된 학습률(Learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "blind-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps = 4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model      = d_model\n",
    "        self.d_model      = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "medical-negotiation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 커스텀된 학습률 시각화 해보기\n",
    "temp_learning_rate = CustomSchedule(d_model = 128)\n",
    "\n",
    "plt.plot(temp_learning_rate(tf.range(200000, dtype = tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-request",
   "metadata": {},
   "source": [
    "### 11_4. 모델 컴파일 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "relative-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "optimizer     = tf.keras.optimizers.Adam(learning_rate, beta_1 = 0.9, beta_2 = 0.98, epsilon = 1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape = (-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = loss_function, metrics = [accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-elimination",
   "metadata": {},
   "source": [
    "### 11_5. 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "moved-convert",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "185/185 [==============================] - 38s 206ms/step - loss: 1.3446 - accuracy: 0.0230\n",
      "Epoch 2/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 1.0713 - accuracy: 0.0492\n",
      "Epoch 3/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.9809 - accuracy: 0.0503\n",
      "Epoch 4/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.9362 - accuracy: 0.0524\n",
      "Epoch 5/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.9015 - accuracy: 0.0546\n",
      "Epoch 6/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.8651 - accuracy: 0.0565\n",
      "Epoch 7/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.8251 - accuracy: 0.0592\n",
      "Epoch 8/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.7776 - accuracy: 0.0618\n",
      "Epoch 9/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.7253 - accuracy: 0.0658\n",
      "Epoch 10/400\n",
      "185/185 [==============================] - 36s 197ms/step - loss: 0.6699 - accuracy: 0.0705\n",
      "Epoch 11/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.6136 - accuracy: 0.0767\n",
      "Epoch 12/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.5555 - accuracy: 0.0834\n",
      "Epoch 13/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.5027 - accuracy: 0.0899\n",
      "Epoch 14/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.4507 - accuracy: 0.0968\n",
      "Epoch 15/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.4098 - accuracy: 0.1020\n",
      "Epoch 16/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.3713 - accuracy: 0.1073\n",
      "Epoch 17/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.3407 - accuracy: 0.1113\n",
      "Epoch 18/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.3175 - accuracy: 0.1146\n",
      "Epoch 19/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.3016 - accuracy: 0.1164\n",
      "Epoch 20/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.2880 - accuracy: 0.1185\n",
      "Epoch 21/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.2772 - accuracy: 0.1201\n",
      "Epoch 22/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.2709 - accuracy: 0.1209\n",
      "Epoch 23/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.2582 - accuracy: 0.1229\n",
      "Epoch 24/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.2440 - accuracy: 0.1254\n",
      "Epoch 25/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.2318 - accuracy: 0.1273\n",
      "Epoch 26/400\n",
      "185/185 [==============================] - 36s 197ms/step - loss: 0.2231 - accuracy: 0.1288\n",
      "Epoch 27/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.2119 - accuracy: 0.1306\n",
      "Epoch 28/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.2049 - accuracy: 0.1316\n",
      "Epoch 29/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.1982 - accuracy: 0.1326\n",
      "Epoch 30/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.1895 - accuracy: 0.1339\n",
      "Epoch 31/400\n",
      "185/185 [==============================] - 36s 197ms/step - loss: 0.1836 - accuracy: 0.1348\n",
      "Epoch 32/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.1782 - accuracy: 0.1356\n",
      "Epoch 33/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.1722 - accuracy: 0.1367\n",
      "Epoch 34/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.1669 - accuracy: 0.1373\n",
      "Epoch 35/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.1620 - accuracy: 0.1382\n",
      "Epoch 36/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.1561 - accuracy: 0.1390\n",
      "Epoch 37/400\n",
      "185/185 [==============================] - 36s 197ms/step - loss: 0.1527 - accuracy: 0.1397\n",
      "Epoch 38/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.1471 - accuracy: 0.1405\n",
      "Epoch 39/400\n",
      "185/185 [==============================] - 37s 201ms/step - loss: 0.1429 - accuracy: 0.1411\n",
      "Epoch 40/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.1385 - accuracy: 0.1421\n",
      "Epoch 41/400\n",
      "185/185 [==============================] - 37s 199ms/step - loss: 0.1364 - accuracy: 0.1422\n",
      "Epoch 42/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.1313 - accuracy: 0.1431\n",
      "Epoch 43/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.1281 - accuracy: 0.1438\n",
      "Epoch 44/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.1240 - accuracy: 0.1445\n",
      "Epoch 45/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.1219 - accuracy: 0.1447\n",
      "Epoch 46/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.1177 - accuracy: 0.1452\n",
      "Epoch 47/400\n",
      "185/185 [==============================] - 38s 208ms/step - loss: 0.1143 - accuracy: 0.1461\n",
      "Epoch 48/400\n",
      "185/185 [==============================] - 38s 207ms/step - loss: 0.1131 - accuracy: 0.1460\n",
      "Epoch 49/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.1105 - accuracy: 0.1468\n",
      "Epoch 50/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.1065 - accuracy: 0.1474\n",
      "Epoch 51/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.1057 - accuracy: 0.1476\n",
      "Epoch 52/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.1022 - accuracy: 0.1482\n",
      "Epoch 53/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.1004 - accuracy: 0.1487\n",
      "Epoch 54/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0984 - accuracy: 0.1491\n",
      "Epoch 55/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0960 - accuracy: 0.1496\n",
      "Epoch 56/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0936 - accuracy: 0.1498\n",
      "Epoch 57/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0927 - accuracy: 0.1503\n",
      "Epoch 58/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0903 - accuracy: 0.1506\n",
      "Epoch 59/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0883 - accuracy: 0.1511\n",
      "Epoch 60/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0865 - accuracy: 0.1514\n",
      "Epoch 61/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0849 - accuracy: 0.1519\n",
      "Epoch 62/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0833 - accuracy: 0.1522\n",
      "Epoch 63/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0810 - accuracy: 0.1528\n",
      "Epoch 64/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0804 - accuracy: 0.1527\n",
      "Epoch 65/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0782 - accuracy: 0.1532\n",
      "Epoch 66/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0769 - accuracy: 0.1536\n",
      "Epoch 67/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0763 - accuracy: 0.1536\n",
      "Epoch 68/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0745 - accuracy: 0.1540\n",
      "Epoch 69/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0731 - accuracy: 0.1545\n",
      "Epoch 70/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0715 - accuracy: 0.1548\n",
      "Epoch 71/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0712 - accuracy: 0.1550\n",
      "Epoch 72/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0694 - accuracy: 0.1553\n",
      "Epoch 73/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0681 - accuracy: 0.1555\n",
      "Epoch 74/400\n",
      "185/185 [==============================] - 36s 192ms/step - loss: 0.0663 - accuracy: 0.1560\n",
      "Epoch 75/400\n",
      "185/185 [==============================] - 36s 192ms/step - loss: 0.0647 - accuracy: 0.1563\n",
      "Epoch 76/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0643 - accuracy: 0.1565\n",
      "Epoch 77/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0642 - accuracy: 0.1566\n",
      "Epoch 78/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0619 - accuracy: 0.1570\n",
      "Epoch 79/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0610 - accuracy: 0.1574\n",
      "Epoch 80/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0590 - accuracy: 0.1576\n",
      "Epoch 81/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0584 - accuracy: 0.1579\n",
      "Epoch 82/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0579 - accuracy: 0.1579\n",
      "Epoch 83/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0584 - accuracy: 0.1578\n",
      "Epoch 84/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0551 - accuracy: 0.1588\n",
      "Epoch 85/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0556 - accuracy: 0.1585\n",
      "Epoch 86/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0548 - accuracy: 0.1585\n",
      "Epoch 87/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0532 - accuracy: 0.1592\n",
      "Epoch 88/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0530 - accuracy: 0.1591\n",
      "Epoch 89/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0526 - accuracy: 0.1590\n",
      "Epoch 90/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0521 - accuracy: 0.1594\n",
      "Epoch 91/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0502 - accuracy: 0.1599\n",
      "Epoch 92/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0493 - accuracy: 0.1601\n",
      "Epoch 93/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0487 - accuracy: 0.1603\n",
      "Epoch 94/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0483 - accuracy: 0.1604\n",
      "Epoch 95/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0472 - accuracy: 0.1606\n",
      "Epoch 96/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0483 - accuracy: 0.1603\n",
      "Epoch 97/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0463 - accuracy: 0.1610\n",
      "Epoch 98/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0450 - accuracy: 0.1611\n",
      "Epoch 99/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0455 - accuracy: 0.1612\n",
      "Epoch 100/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0441 - accuracy: 0.1615\n",
      "Epoch 101/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0427 - accuracy: 0.1619\n",
      "Epoch 102/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0429 - accuracy: 0.1617\n",
      "Epoch 103/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0421 - accuracy: 0.1621\n",
      "Epoch 104/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0419 - accuracy: 0.1621\n",
      "Epoch 105/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0414 - accuracy: 0.1624\n",
      "Epoch 106/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0408 - accuracy: 0.1624\n",
      "Epoch 107/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0400 - accuracy: 0.1626\n",
      "Epoch 108/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0386 - accuracy: 0.1628\n",
      "Epoch 109/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0389 - accuracy: 0.1628\n",
      "Epoch 110/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0380 - accuracy: 0.1631\n",
      "Epoch 111/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0379 - accuracy: 0.1631\n",
      "Epoch 112/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0378 - accuracy: 0.1631\n",
      "Epoch 113/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0369 - accuracy: 0.1634\n",
      "Epoch 114/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0366 - accuracy: 0.1635\n",
      "Epoch 115/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0360 - accuracy: 0.1637\n",
      "Epoch 116/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0358 - accuracy: 0.1638\n",
      "Epoch 117/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0351 - accuracy: 0.1637\n",
      "Epoch 118/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0351 - accuracy: 0.1639\n",
      "Epoch 119/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0348 - accuracy: 0.1641\n",
      "Epoch 120/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0340 - accuracy: 0.1641\n",
      "Epoch 121/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0337 - accuracy: 0.1642\n",
      "Epoch 122/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0321 - accuracy: 0.1647\n",
      "Epoch 123/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0331 - accuracy: 0.1645\n",
      "Epoch 124/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0325 - accuracy: 0.1646\n",
      "Epoch 125/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0323 - accuracy: 0.1646\n",
      "Epoch 126/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0314 - accuracy: 0.1648\n",
      "Epoch 127/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0317 - accuracy: 0.1648\n",
      "Epoch 128/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0313 - accuracy: 0.1649\n",
      "Epoch 129/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0301 - accuracy: 0.1652\n",
      "Epoch 130/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0307 - accuracy: 0.1651\n",
      "Epoch 131/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0303 - accuracy: 0.1653\n",
      "Epoch 132/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0297 - accuracy: 0.1653\n",
      "Epoch 133/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0293 - accuracy: 0.1655\n",
      "Epoch 134/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0296 - accuracy: 0.1654\n",
      "Epoch 135/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0286 - accuracy: 0.1658\n",
      "Epoch 136/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0287 - accuracy: 0.1657\n",
      "Epoch 137/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0286 - accuracy: 0.1657\n",
      "Epoch 138/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0283 - accuracy: 0.1659\n",
      "Epoch 139/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0276 - accuracy: 0.1659\n",
      "Epoch 140/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0268 - accuracy: 0.1663\n",
      "Epoch 141/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0267 - accuracy: 0.1662\n",
      "Epoch 142/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0266 - accuracy: 0.1663\n",
      "Epoch 143/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0261 - accuracy: 0.1663\n",
      "Epoch 144/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0259 - accuracy: 0.1664\n",
      "Epoch 145/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0255 - accuracy: 0.1665\n",
      "Epoch 146/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0256 - accuracy: 0.1665\n",
      "Epoch 147/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0251 - accuracy: 0.1666\n",
      "Epoch 148/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0247 - accuracy: 0.1667\n",
      "Epoch 149/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0247 - accuracy: 0.1668\n",
      "Epoch 150/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0236 - accuracy: 0.1671\n",
      "Epoch 151/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0240 - accuracy: 0.1670\n",
      "Epoch 152/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0227 - accuracy: 0.1674\n",
      "Epoch 153/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0239 - accuracy: 0.1671\n",
      "Epoch 154/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0231 - accuracy: 0.1672\n",
      "Epoch 155/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0232 - accuracy: 0.1672\n",
      "Epoch 156/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0228 - accuracy: 0.1674\n",
      "Epoch 157/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0224 - accuracy: 0.1676\n",
      "Epoch 158/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0232 - accuracy: 0.1672\n",
      "Epoch 159/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0221 - accuracy: 0.1675\n",
      "Epoch 160/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0220 - accuracy: 0.1676\n",
      "Epoch 161/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0217 - accuracy: 0.1676\n",
      "Epoch 162/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.0219 - accuracy: 0.1675\n",
      "Epoch 163/400\n",
      "185/185 [==============================] - 38s 208ms/step - loss: 0.0213 - accuracy: 0.1678\n",
      "Epoch 164/400\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.0213 - accuracy: 0.1679\n",
      "Epoch 165/400\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.0223 - accuracy: 0.1677\n",
      "Epoch 166/400\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.0201 - accuracy: 0.1682\n",
      "Epoch 167/400\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.0205 - accuracy: 0.1680\n",
      "Epoch 168/400\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.0201 - accuracy: 0.1680\n",
      "Epoch 169/400\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.0203 - accuracy: 0.1680\n",
      "Epoch 170/400\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.0197 - accuracy: 0.1682\n",
      "Epoch 171/400\n",
      "185/185 [==============================] - 38s 203ms/step - loss: 0.0201 - accuracy: 0.1681\n",
      "Epoch 172/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0193 - accuracy: 0.1683\n",
      "Epoch 173/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0200 - accuracy: 0.1682\n",
      "Epoch 174/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0199 - accuracy: 0.1684\n",
      "Epoch 175/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0195 - accuracy: 0.1682\n",
      "Epoch 176/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0185 - accuracy: 0.1686\n",
      "Epoch 177/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0182 - accuracy: 0.1687\n",
      "Epoch 178/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0191 - accuracy: 0.1685\n",
      "Epoch 179/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0187 - accuracy: 0.1687\n",
      "Epoch 180/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0188 - accuracy: 0.1685\n",
      "Epoch 181/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0187 - accuracy: 0.1687\n",
      "Epoch 182/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0179 - accuracy: 0.1687\n",
      "Epoch 183/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0177 - accuracy: 0.1688\n",
      "Epoch 184/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0170 - accuracy: 0.1690\n",
      "Epoch 185/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0179 - accuracy: 0.1687\n",
      "Epoch 186/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0171 - accuracy: 0.1690\n",
      "Epoch 187/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0170 - accuracy: 0.1691\n",
      "Epoch 188/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0166 - accuracy: 0.1691\n",
      "Epoch 189/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0170 - accuracy: 0.1690\n",
      "Epoch 190/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0167 - accuracy: 0.1692\n",
      "Epoch 191/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0165 - accuracy: 0.1692\n",
      "Epoch 192/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0166 - accuracy: 0.1691\n",
      "Epoch 193/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0167 - accuracy: 0.1691\n",
      "Epoch 194/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0168 - accuracy: 0.1692\n",
      "Epoch 195/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0166 - accuracy: 0.1692\n",
      "Epoch 196/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0163 - accuracy: 0.1692\n",
      "Epoch 197/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0158 - accuracy: 0.1694\n",
      "Epoch 198/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0161 - accuracy: 0.1693\n",
      "Epoch 199/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0159 - accuracy: 0.1694\n",
      "Epoch 200/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0156 - accuracy: 0.1694\n",
      "Epoch 201/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0159 - accuracy: 0.1694\n",
      "Epoch 202/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0153 - accuracy: 0.1696\n",
      "Epoch 203/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0153 - accuracy: 0.1695\n",
      "Epoch 204/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0161 - accuracy: 0.1694\n",
      "Epoch 205/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0147 - accuracy: 0.1698\n",
      "Epoch 206/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0154 - accuracy: 0.1696\n",
      "Epoch 207/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0149 - accuracy: 0.1697\n",
      "Epoch 208/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0146 - accuracy: 0.1697\n",
      "Epoch 209/400\n",
      "185/185 [==============================] - 36s 192ms/step - loss: 0.0144 - accuracy: 0.1698\n",
      "Epoch 210/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0146 - accuracy: 0.1697\n",
      "Epoch 211/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0142 - accuracy: 0.1698\n",
      "Epoch 212/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0141 - accuracy: 0.1700\n",
      "Epoch 213/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0142 - accuracy: 0.1699\n",
      "Epoch 214/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0137 - accuracy: 0.1700\n",
      "Epoch 215/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0142 - accuracy: 0.1699\n",
      "Epoch 216/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0137 - accuracy: 0.1700\n",
      "Epoch 217/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0139 - accuracy: 0.1700\n",
      "Epoch 218/400\n",
      "185/185 [==============================] - 35s 191ms/step - loss: 0.0136 - accuracy: 0.1700\n",
      "Epoch 219/400\n",
      "185/185 [==============================] - 35s 192ms/step - loss: 0.0133 - accuracy: 0.1702\n",
      "Epoch 220/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0141 - accuracy: 0.1700\n",
      "Epoch 221/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0132 - accuracy: 0.1701\n",
      "Epoch 222/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0131 - accuracy: 0.1701\n",
      "Epoch 223/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0136 - accuracy: 0.1701\n",
      "Epoch 224/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0128 - accuracy: 0.1703\n",
      "Epoch 225/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0133 - accuracy: 0.1702\n",
      "Epoch 226/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.0127 - accuracy: 0.1703\n",
      "Epoch 227/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0130 - accuracy: 0.1703\n",
      "Epoch 228/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0124 - accuracy: 0.1704\n",
      "Epoch 229/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0121 - accuracy: 0.1705\n",
      "Epoch 230/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0127 - accuracy: 0.1702\n",
      "Epoch 231/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0131 - accuracy: 0.1702\n",
      "Epoch 232/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0126 - accuracy: 0.1704\n",
      "Epoch 233/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0119 - accuracy: 0.1706\n",
      "Epoch 234/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0123 - accuracy: 0.1705\n",
      "Epoch 235/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0126 - accuracy: 0.1703\n",
      "Epoch 236/400\n",
      "185/185 [==============================] - 36s 192ms/step - loss: 0.0127 - accuracy: 0.1703\n",
      "Epoch 237/400\n",
      "185/185 [==============================] - 36s 192ms/step - loss: 0.0119 - accuracy: 0.1706\n",
      "Epoch 238/400\n",
      "185/185 [==============================] - 37s 198ms/step - loss: 0.0121 - accuracy: 0.1705\n",
      "Epoch 239/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0115 - accuracy: 0.1706\n",
      "Epoch 240/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.0118 - accuracy: 0.1706\n",
      "Epoch 241/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0113 - accuracy: 0.1707\n",
      "Epoch 242/400\n",
      "185/185 [==============================] - 37s 198ms/step - loss: 0.0111 - accuracy: 0.1708\n",
      "Epoch 243/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0116 - accuracy: 0.1707\n",
      "Epoch 244/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0115 - accuracy: 0.1707\n",
      "Epoch 245/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0117 - accuracy: 0.1708\n",
      "Epoch 246/400\n",
      "185/185 [==============================] - 39s 209ms/step - loss: 0.0109 - accuracy: 0.1708\n",
      "Epoch 247/400\n",
      "185/185 [==============================] - 37s 199ms/step - loss: 0.0114 - accuracy: 0.1706\n",
      "Epoch 248/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0111 - accuracy: 0.1709\n",
      "Epoch 249/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0112 - accuracy: 0.1707\n",
      "Epoch 250/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0114 - accuracy: 0.1708\n",
      "Epoch 251/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0109 - accuracy: 0.1709\n",
      "Epoch 252/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0110 - accuracy: 0.1708\n",
      "Epoch 253/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0106 - accuracy: 0.1709\n",
      "Epoch 254/400\n",
      "185/185 [==============================] - 38s 203ms/step - loss: 0.0109 - accuracy: 0.1709\n",
      "Epoch 255/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0119 - accuracy: 0.1707\n",
      "Epoch 256/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0107 - accuracy: 0.1708\n",
      "Epoch 257/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0109 - accuracy: 0.1709\n",
      "Epoch 258/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0111 - accuracy: 0.1708\n",
      "Epoch 259/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0107 - accuracy: 0.1711\n",
      "Epoch 260/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0103 - accuracy: 0.1710\n",
      "Epoch 261/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0104 - accuracy: 0.1710\n",
      "Epoch 262/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0102 - accuracy: 0.1711\n",
      "Epoch 263/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.0098 - accuracy: 0.1712\n",
      "Epoch 264/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0106 - accuracy: 0.1709\n",
      "Epoch 265/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0103 - accuracy: 0.1710\n",
      "Epoch 266/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0102 - accuracy: 0.1711\n",
      "Epoch 267/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0100 - accuracy: 0.1711s - l\n",
      "Epoch 268/400\n",
      "185/185 [==============================] - 38s 204ms/step - loss: 0.0102 - accuracy: 0.1711\n",
      "Epoch 269/400\n",
      "185/185 [==============================] - 37s 200ms/step - loss: 0.0101 - accuracy: 0.1711\n",
      "Epoch 270/400\n",
      "185/185 [==============================] - 36s 197ms/step - loss: 0.0099 - accuracy: 0.1712\n",
      "Epoch 271/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0098 - accuracy: 0.1712\n",
      "Epoch 272/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0099 - accuracy: 0.1712\n",
      "Epoch 273/400\n",
      "185/185 [==============================] - 38s 203ms/step - loss: 0.0102 - accuracy: 0.1711\n",
      "Epoch 274/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0092 - accuracy: 0.1713\n",
      "Epoch 275/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0094 - accuracy: 0.1713\n",
      "Epoch 276/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0102 - accuracy: 0.1711\n",
      "Epoch 277/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0096 - accuracy: 0.1712\n",
      "Epoch 278/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0095 - accuracy: 0.1713\n",
      "Epoch 279/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0095 - accuracy: 0.1713\n",
      "Epoch 280/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0093 - accuracy: 0.1714\n",
      "Epoch 281/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.0094 - accuracy: 0.1713\n",
      "Epoch 282/400\n",
      "185/185 [==============================] - 37s 198ms/step - loss: 0.0095 - accuracy: 0.1713\n",
      "Epoch 283/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.0091 - accuracy: 0.1714\n",
      "Epoch 284/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0091 - accuracy: 0.1714\n",
      "Epoch 285/400\n",
      "185/185 [==============================] - 37s 198ms/step - loss: 0.0094 - accuracy: 0.1714\n",
      "Epoch 286/400\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.0089 - accuracy: 0.1715\n",
      "Epoch 287/400\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.0092 - accuracy: 0.1714\n",
      "Epoch 288/400\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.0091 - accuracy: 0.1713\n",
      "Epoch 289/400\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.0091 - accuracy: 0.1713\n",
      "Epoch 290/400\n",
      "185/185 [==============================] - 39s 213ms/step - loss: 0.0091 - accuracy: 0.1714\n",
      "Epoch 291/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0093 - accuracy: 0.1713\n",
      "Epoch 292/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0088 - accuracy: 0.1715\n",
      "Epoch 293/400\n",
      "185/185 [==============================] - 38s 203ms/step - loss: 0.0086 - accuracy: 0.1716\n",
      "Epoch 294/400\n",
      "185/185 [==============================] - 38s 208ms/step - loss: 0.0088 - accuracy: 0.1714\n",
      "Epoch 295/400\n",
      "185/185 [==============================] - 38s 208ms/step - loss: 0.0087 - accuracy: 0.1716\n",
      "Epoch 296/400\n",
      "185/185 [==============================] - 39s 210ms/step - loss: 0.0083 - accuracy: 0.1716\n",
      "Epoch 297/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0084 - accuracy: 0.1716\n",
      "Epoch 298/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0081 - accuracy: 0.1716\n",
      "Epoch 299/400\n",
      "185/185 [==============================] - 37s 199ms/step - loss: 0.0087 - accuracy: 0.1715\n",
      "Epoch 300/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0087 - accuracy: 0.1715\n",
      "Epoch 301/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0080 - accuracy: 0.1717\n",
      "Epoch 302/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0085 - accuracy: 0.1716\n",
      "Epoch 303/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.0082 - accuracy: 0.1717\n",
      "Epoch 304/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0086 - accuracy: 0.1716\n",
      "Epoch 305/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0080 - accuracy: 0.1717\n",
      "Epoch 306/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0080 - accuracy: 0.1716\n",
      "Epoch 307/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0082 - accuracy: 0.1717\n",
      "Epoch 308/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0077 - accuracy: 0.1718\n",
      "Epoch 309/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0083 - accuracy: 0.1717\n",
      "Epoch 310/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0080 - accuracy: 0.1717\n",
      "Epoch 311/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0082 - accuracy: 0.1716\n",
      "Epoch 312/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0079 - accuracy: 0.1716\n",
      "Epoch 313/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0082 - accuracy: 0.1716\n",
      "Epoch 314/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0080 - accuracy: 0.1717\n",
      "Epoch 315/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0076 - accuracy: 0.1718\n",
      "Epoch 316/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0079 - accuracy: 0.1718\n",
      "Epoch 317/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0075 - accuracy: 0.1718\n",
      "Epoch 318/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0079 - accuracy: 0.1718\n",
      "Epoch 319/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0071 - accuracy: 0.1719\n",
      "Epoch 320/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0082 - accuracy: 0.1717\n",
      "Epoch 321/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0079 - accuracy: 0.1717\n",
      "Epoch 322/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0071 - accuracy: 0.1720\n",
      "Epoch 323/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.0073 - accuracy: 0.1720\n",
      "Epoch 324/400\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.0074 - accuracy: 0.1719\n",
      "Epoch 325/400\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.0074 - accuracy: 0.1719\n",
      "Epoch 326/400\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.0071 - accuracy: 0.1720\n",
      "Epoch 327/400\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.0071 - accuracy: 0.1720\n",
      "Epoch 328/400\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.0077 - accuracy: 0.1718\n",
      "Epoch 329/400\n",
      "185/185 [==============================] - 41s 224ms/step - loss: 0.0072 - accuracy: 0.1719\n",
      "Epoch 330/400\n",
      "185/185 [==============================] - 41s 223ms/step - loss: 0.0073 - accuracy: 0.1720\n",
      "Epoch 331/400\n",
      "185/185 [==============================] - 41s 224ms/step - loss: 0.0074 - accuracy: 0.1719\n",
      "Epoch 332/400\n",
      "185/185 [==============================] - 41s 224ms/step - loss: 0.0068 - accuracy: 0.1721\n",
      "Epoch 333/400\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.0069 - accuracy: 0.1721\n",
      "Epoch 334/400\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.0064 - accuracy: 0.1722\n",
      "Epoch 335/400\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.0072 - accuracy: 0.1720\n",
      "Epoch 336/400\n",
      "185/185 [==============================] - 37s 201ms/step - loss: 0.0071 - accuracy: 0.1719\n",
      "Epoch 337/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0068 - accuracy: 0.1720\n",
      "Epoch 338/400\n",
      "185/185 [==============================] - 37s 199ms/step - loss: 0.0068 - accuracy: 0.1720\n",
      "Epoch 339/400\n",
      "185/185 [==============================] - 39s 210ms/step - loss: 0.0069 - accuracy: 0.1720\n",
      "Epoch 340/400\n",
      "185/185 [==============================] - 41s 223ms/step - loss: 0.0069 - accuracy: 0.1720\n",
      "Epoch 341/400\n",
      "185/185 [==============================] - 41s 223ms/step - loss: 0.0067 - accuracy: 0.1721\n",
      "Epoch 342/400\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.0070 - accuracy: 0.1720\n",
      "Epoch 343/400\n",
      "185/185 [==============================] - 41s 223ms/step - loss: 0.0066 - accuracy: 0.1720\n",
      "Epoch 344/400\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.0067 - accuracy: 0.1720\n",
      "Epoch 345/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0070 - accuracy: 0.1720\n",
      "Epoch 346/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0066 - accuracy: 0.1722\n",
      "Epoch 347/400\n",
      "185/185 [==============================] - 38s 203ms/step - loss: 0.0068 - accuracy: 0.1721\n",
      "Epoch 348/400\n",
      "185/185 [==============================] - 37s 198ms/step - loss: 0.0066 - accuracy: 0.1721\n",
      "Epoch 349/400\n",
      "185/185 [==============================] - 37s 198ms/step - loss: 0.0067 - accuracy: 0.1721\n",
      "Epoch 350/400\n",
      "185/185 [==============================] - 38s 203ms/step - loss: 0.0065 - accuracy: 0.1722\n",
      "Epoch 351/400\n",
      "185/185 [==============================] - 37s 198ms/step - loss: 0.0065 - accuracy: 0.1722\n",
      "Epoch 352/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0062 - accuracy: 0.1722\n",
      "Epoch 353/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0061 - accuracy: 0.1722\n",
      "Epoch 354/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0063 - accuracy: 0.1722\n",
      "Epoch 355/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.0065 - accuracy: 0.1721\n",
      "Epoch 356/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0061 - accuracy: 0.1723\n",
      "Epoch 357/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0062 - accuracy: 0.1722\n",
      "Epoch 358/400\n",
      "185/185 [==============================] - 37s 198ms/step - loss: 0.0065 - accuracy: 0.1722\n",
      "Epoch 359/400\n",
      "185/185 [==============================] - 37s 200ms/step - loss: 0.0063 - accuracy: 0.1722\n",
      "Epoch 360/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0061 - accuracy: 0.1723\n",
      "Epoch 361/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0065 - accuracy: 0.1721\n",
      "Epoch 362/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0062 - accuracy: 0.1723\n",
      "Epoch 363/400\n",
      "185/185 [==============================] - 37s 199ms/step - loss: 0.0062 - accuracy: 0.1723\n",
      "Epoch 364/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.0065 - accuracy: 0.1722\n",
      "Epoch 365/400\n",
      "185/185 [==============================] - 36s 197ms/step - loss: 0.0060 - accuracy: 0.1723\n",
      "Epoch 366/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.0062 - accuracy: 0.1721\n",
      "Epoch 367/400\n",
      "185/185 [==============================] - 36s 197ms/step - loss: 0.0062 - accuracy: 0.1723\n",
      "Epoch 368/400\n",
      "185/185 [==============================] - 36s 197ms/step - loss: 0.0060 - accuracy: 0.1723\n",
      "Epoch 369/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.0058 - accuracy: 0.1723\n",
      "Epoch 370/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.0060 - accuracy: 0.1724\n",
      "Epoch 371/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.0060 - accuracy: 0.1722\n",
      "Epoch 372/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.0056 - accuracy: 0.1724\n",
      "Epoch 373/400\n",
      "185/185 [==============================] - 36s 192ms/step - loss: 0.0054 - accuracy: 0.1725\n",
      "Epoch 374/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0059 - accuracy: 0.1723\n",
      "Epoch 375/400\n",
      "185/185 [==============================] - 36s 192ms/step - loss: 0.0060 - accuracy: 0.1723\n",
      "Epoch 376/400\n",
      "185/185 [==============================] - 36s 192ms/step - loss: 0.0058 - accuracy: 0.1723\n",
      "Epoch 377/400\n",
      "185/185 [==============================] - 36s 193ms/step - loss: 0.0057 - accuracy: 0.1724\n",
      "Epoch 378/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0056 - accuracy: 0.1724\n",
      "Epoch 379/400\n",
      "185/185 [==============================] - 36s 192ms/step - loss: 0.0054 - accuracy: 0.1725\n",
      "Epoch 380/400\n",
      "185/185 [==============================] - 36s 196ms/step - loss: 0.0060 - accuracy: 0.1722\n",
      "Epoch 381/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0058 - accuracy: 0.1724\n",
      "Epoch 382/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0055 - accuracy: 0.1724\n",
      "Epoch 383/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0060 - accuracy: 0.1724\n",
      "Epoch 384/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0058 - accuracy: 0.1723\n",
      "Epoch 385/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0058 - accuracy: 0.1723\n",
      "Epoch 386/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0056 - accuracy: 0.1724\n",
      "Epoch 387/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0059 - accuracy: 0.1724\n",
      "Epoch 388/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0053 - accuracy: 0.1725\n",
      "Epoch 389/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0058 - accuracy: 0.1724\n",
      "Epoch 390/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0054 - accuracy: 0.1724\n",
      "Epoch 391/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0058 - accuracy: 0.1723\n",
      "Epoch 392/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0054 - accuracy: 0.1725\n",
      "Epoch 393/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0054 - accuracy: 0.1724\n",
      "Epoch 394/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0059 - accuracy: 0.1724\n",
      "Epoch 395/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0054 - accuracy: 0.1724\n",
      "Epoch 396/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0054 - accuracy: 0.1724\n",
      "Epoch 397/400\n",
      "185/185 [==============================] - 36s 194ms/step - loss: 0.0059 - accuracy: 0.1723\n",
      "Epoch 398/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0053 - accuracy: 0.1726\n",
      "Epoch 399/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0054 - accuracy: 0.1725\n",
      "Epoch 400/400\n",
      "185/185 [==============================] - 36s 195ms/step - loss: 0.0051 - accuracy: 0.1725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fab101aac10>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 400 # 훈련량 정하기\n",
    "model.fit(dataset, epochs = EPOCHS, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-indicator",
   "metadata": {},
   "source": [
    "## 12. 모델 테스트하기\n",
    "#### 예측(inference) 단계는 기본적으로 다음과 같은 과정을 거친다.\n",
    "* 새로운 입력 문장에 대해서는 훈련 때와 동일한 전처리를 거친다.\n",
    "* 입력 문장을 토크나이징하고, START_TOKEN과 END_TOKEN을 추가한다.\n",
    "* 패딩 마스킹과 룩 어헤드 마스킹을 계산한다.\n",
    "* 디코더는 입력 시퀀스로부터 다음 단어를 예측한다.\n",
    "* 디코더는 예측된 다음 단어를 기존의 입력 시퀀스에 추가하여 새로운 입력으로 사용한다.\n",
    "* END_TOKEN이 예측되거나 문장의 최대 길이에 도달하면 디코더는 동작을 멈춘다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "convinced-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 말한 과정을 수행하는 함수 만들기\n",
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가한다.\n",
    "    sentence = tf.expand_dims(START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis = 0)\n",
    "    \n",
    "    # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수\n",
    "    # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "    \n",
    "    # 디코더의 인퍼런스 단계\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복한다.\n",
    "        predictions = model(inputs = [sentence, output_sequence], training = False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        \n",
    "        # 현재 예측한 단어의 정수\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis = -1), tf.int32)\n",
    "        \n",
    "        # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료한다.\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]): break\n",
    "        \n",
    "        # 예측한 단어들은 지속적으로 output_sequence에 추가된다.\n",
    "        # 이 output_sequence는 다시 디코더의 입력이 된다.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis = -1)\n",
    "        \n",
    "    return tf.squeeze(output_sequence, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "accompanied-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 입력 문장에 대해서 위에 만든 함수를 호출하여 챗봇의 대답을 얻는 함수 만들기\n",
    "def sentence_generation(sentence):\n",
    "    # 입력 문장에 대해서 디코더를 작동시켜 예측된 정수 시퀀스를 반환한다.\n",
    "    prediction = decoder_inference(sentence)\n",
    "    \n",
    "    # 정수 시퀀스를 다시 텍스트 시퀀스로 변환한다.\n",
    "    predicted_sentence = tokenizer.decode([i for i in prediction if i < tokenizer.vocab_size])\n",
    "    \n",
    "    print(\"입력 : {}\".format(sentence))\n",
    "    print(\"출력 : {}\".format(predicted_sentence))\n",
    "    \n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "blind-invalid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 퇴근이 가능할까?\n",
      "출력 : 뭘 잘못 먹었나봐요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'뭘 잘못 먹었나봐요 .'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"퇴근이 가능할까?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "pretty-karaoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 저녁을 먹어도 될까?\n",
      "출력 : 맛있는 거 드세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'맛있는 거 드세요 .'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"저녁을 먹어도 될까?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "senior-forge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 공부하기 싫어\n",
      "출력 : 잠시 쉬어도 돼요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'잠시 쉬어도 돼요 .'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"공부하기 싫어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "undefined-detector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 자기소개 해봐\n",
      "출력 : 강렬하고 자신감있게 하면 되지 않을까싶어요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'강렬하고 자신감있게 하면 되지 않을까싶어요 .'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"자기소개 해봐\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "amber-turkey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 나 머리 아파\n",
      "출력 : 시원한 바람 쉬고 약먹기 !\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'시원한 바람 쉬고 약먹기 !'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"나 머리 아파\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "proud-tyler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 사람은 왜 살아?\n",
      "출력 : 서로 마음이 같지 않았나봐요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'서로 마음이 같지 않았나봐요 .'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"사람은 왜 살아?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "revolutionary-compatibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 위스키 한잔 할래?\n",
      "출력 : 즐거운 시작 되길 바랍니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'즐거운 시작 되길 바랍니다 .'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"위스키 한잔 할래?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-sleeve",
   "metadata": {},
   "source": [
    "## 회고록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번 노드는 개념부터 코딩까지 전부 어려웠다. 전혀 쉬운부분이 없어서 쉬는 타임이 없었다.\n",
    "# 그래서 더욱 힘들었다. 역대 2번쨰... 1번째는 리뷰 감성분석이다.\n",
    "# 개념도 어려워서 2~3번 봤다.\n",
    "# 노드를 보면서 코드를 타이핑 하는 편인데, 코드를 타이핑을 똑같이 했다고 생각했는데도 우리의 챗봇은 아무 대답이 없었다.\n",
    "# 또한 한글 데이터의 전처리는 영어 데이터랑 달라서 코드를 수정하는데 애를 먹었다.\n",
    "# 근데 막상 하다보니 '내가 너무 어렵게 하려 하고 있다' 라는 생각을 하게 되었고, '결국 결과만 같으면 되는 것 아닌가' 라는 생각을 해서 전처리를 했다.\n",
    "# 그리고 코드는 결국 복사, 붙여넣기를 통해 했다. 내가 타이핑한 코드에서 어느 부분이 달라서 작동이 제대로 안 되는지 찾지 못했다.\n",
    "# 그리고 20번 훈련을 시키니 대답을 너무 잘했다. \"나 우울해\" -> \"밖에 나가서 기분을 전환해요\"\n",
    "# 그래서 저녁먹고 복습 겸 블로그 작성을 하는 동안 학습을 시켜보고 싶어서 400번을 훈련시켰다.\n",
    "# 결과는 만족스럽다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
