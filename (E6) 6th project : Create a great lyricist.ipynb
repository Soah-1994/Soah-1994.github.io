{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "excellent-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 로딩하기\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import glob\n",
    "import sklearn.model_selection\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ancient-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "txt_file_path = os.getenv(\"HOME\") + \"/aiffel/lyricist/data/lyrics/*\" # ~/aiffel/lyricist/data/lyrics 폴더 안의 모든 파일 선택\n",
    "txt_list      = glob.glob(txt_file_path)                             # 파일 경로 안의 모든 파일 이름 리스트형태로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "jewish-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = []                     # 미리 빈 리스트를 만들고\n",
    "\n",
    "for txt_file in txt_list:           # txt_list을 하나씩 가져와서 txt_file이라고 명명하고\n",
    "    with open(txt_file, \"r\") as f:  # txt_file을 읽기전용으로 열고 f라고 명명하고\n",
    "        raw = f.read().splitlines() # f을 한줄씩 읽어와서\n",
    "        raw_corpus.extend(raw)      # 미리 만들어 놓은 raw_corpus에 넣는다. (iterable로 넣기위해 extend 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "secure-antarctica",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문장 갯수 : 187088\n",
      "예시 문장 5개 : ['The first words that come out', 'And I can see this song will be about you', \"I can't believe that I can breathe without you\", 'But all I need to do is carry on', 'The next line I write down']\n"
     ]
    }
   ],
   "source": [
    "print(\"전체 문장 갯수 :\", len(raw_corpus))\n",
    "print(\"예시 문장 5개 :\", raw_corpus[:5])\n",
    "\n",
    "# 연극 대사처럼 따로 빼거나 없애야 하는 특수문자는 보이지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alive-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정제하기 (데이터 전처리)\n",
    "# 토큰화 했을 때 토큰 갯수가 15개를 넘어가는 문장은 학습데이터에서 제외함. (노드에서 시킴)\n",
    "def preprocess_sentence(sentence):      # 문장을 정제하는 함수를 만들 것임\n",
    "    sentence = sentence.lower().strip() # 먼저 들어온 문장을 소문자로 바꾸고 양쪽 공백이 있으면 제거함\n",
    "    \n",
    "    # 문장에서 처리해줘야 할 기호나 패턴을 처리\n",
    "    # 실습 데이터는 그대로 써도 될 것같지만 혹시 몰라서 노드에 있는 경우의 수를 그대로 씀\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)   # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)          # 공백 패턴을 만나면 공백 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 알파벳(대,소문자), ?.!,¿패턴을 제외한 모든 문자(공백 포함)를 공백 1개로 치환\n",
    "    \n",
    "    sentence = sentence.strip()                          # 한번 더 양쪽 공백 제거\n",
    "    sentence = \"<start> \" + sentence + \" <end>\"          # sentence 양쪽에 시작과 끝을 알리는 단어를 붙여줌\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "logical-basket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> test it s sample text <end>\n"
     ]
    }
   ],
   "source": [
    "# 함수가 잘 만들어졌는지 확인하기\n",
    "print(preprocess_sentence(\"Test : It's @#%^$#%$ sample     text\"))\n",
    "\n",
    "# 공백, 특수문자 전부 잘 처리됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "biological-makeup",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> the first words that come out <end>',\n",
       " '<start> and i can see this song will be about you <end>',\n",
       " '<start> i can t believe that i can breathe without you <end>',\n",
       " '<start> but all i need to do is carry on <end>',\n",
       " '<start> the next line i write down <end>',\n",
       " '<start> and there s a tear that falls between the pages <end>',\n",
       " '<start> i know that pain s supposed to heal in stages <end>',\n",
       " '<start> i could throw it in the river and watch it sink in slowly <end>',\n",
       " '<start> tie the pages to a plane and send it to the moon <end>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [] # 빈 리스트를 미리 만들고\n",
    "\n",
    "for sentence in raw_corpus:                       # 기존의 텍스트 데이터를 한개씩 가져와서\n",
    "    if len(sentence) == 0: continue               # 아무것도 없는 것은 제외하고\n",
    "    temp_sentence = preprocess_sentence(sentence) # 정제하는 함수를 거친 뒤\n",
    "    if len(temp_sentence.split()) > 15: continue  # 토큰의 갯수가 15개를 넘어가는 문장은 데이터에서 제외하고\n",
    "    corpus.append(temp_sentence)                  # 리스트에 추가한다.\n",
    "\n",
    "corpus[:9] # 잘 뽑혔는지 10개만 출력하여 확인하기\n",
    "\n",
    "# 잘 작동함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mediterranean-union",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus 갯수 : 156227\n"
     ]
    }
   ],
   "source": [
    "print(\"corpus 갯수 :\", len(corpus)) # 156200이 나와야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wrapped-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정제한 데이터를 텐서(벡터화)로 만들기\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = 12000,   # 전체 단어의 개수 - 하이퍼파라미터\n",
    "                                                      filters   = \" \",     # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "                                                      oov_token = \"<unk>\") # 사전에 없었던 단어는 어떤 토큰으로 대체할지 정한다.\n",
    "    tokenizer.fit_on_texts(corpus) # corpus로부터 tokenizer가 사전을 자동 구축한다.\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(corpus) # 구축한 사전으로부터 corpus를 해석해 tensor로 변환한다.\n",
    "    \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공한다. \n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding = \"post\")\n",
    "    \n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "coordinate-trustee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156227, 15)\n"
     ]
    }
   ],
   "source": [
    "tensor, tokenizer = tokenize(corpus) # 전처리를 한 데이터를 만든 함수에 적용시켜서 텐서로 만든다.\n",
    "print(tensor.shape)                  # 156227개의 문장이 있고, 문장의 토큰 최대길이는 15이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "impossible-lunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2   6 247 434  15  68  57   3   0   0   0   0   0   0   0]\n",
      " [  2   8   4  35  63  41 357  84  27 112   7   3   0   0   0]\n",
      " [  2   4  35  16 218  15   4  35 768 257   7   3   0   0   0]\n",
      " [  2  33  25   4  92  10  48  26 832  18   3   0   0   0   0]\n",
      " [  2   6 330 442   4 760  58   3   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:5, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "comparable-recycling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2   6 247 434  15  68  57   3   0   0   0   0   0   0]\n",
      "[  6 247 434  15  68  57   3   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# 평가 데이터셋 분리\n",
    "source_input = tensor[:, :-1] # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "target_input = tensor[:, 1:]  # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(source_input[0])\n",
    "print(target_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "standing-funds",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124981, 14)\n",
      "Target Train: (124981, 14)\n"
     ]
    }
   ],
   "source": [
    "# sklear를 이용하여 train data와 vaildation data로 나눈다.\n",
    "enc_train, enc_val, dec_train, dec_val = sklearn.model_selection.train_test_split(source_input, target_input, test_size = 0.2)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unnecessary-softball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31246, 14)\n",
      "(31246, 14)\n"
     ]
    }
   ],
   "source": [
    "print(enc_val.shape)\n",
    "print(dec_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "novel-accuracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer_size     = len(source_input)\n",
    "batch_size      = 256\n",
    "steps_per_epoch = buffer_size // batch_size\n",
    "\n",
    "vocab_size = tokenizer.num_words + 1 # tokenizer가 구축한 단어사전 내 12000개 + 여기에 포함되지 않은 0 : <pad>를 포함하여 12001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((source_input, target_input)).shuffle(buffer_size)\n",
    "dataset = dataset.batch(batch_size, drop_remainder = True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "matched-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 만들기\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1     = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.rnn_2     = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.linear    = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-standing",
   "metadata": {},
   "source": [
    "## embedding, hidden을 노드의 2배의 값으로 주고 돌린 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "alpine-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextGenerator(vocab_size = vocab_size, embedding_size = 512, hidden_size = 2048) # embedding_size와 hidden_size를 조절하여 원하는 결과를 도출하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cathedral-integral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-1.0467601e-04, -9.1824593e-05,  4.2911695e-05, ...,\n",
       "         -4.1787236e-05,  2.0358998e-04, -9.4924151e-05],\n",
       "        [-2.9737223e-04, -1.3572344e-04,  3.6873223e-04, ...,\n",
       "          1.9989326e-04,  3.6788877e-04, -1.5870700e-04],\n",
       "        [-7.0538186e-04, -3.1371118e-04,  7.2958518e-04, ...,\n",
       "          2.9374895e-04,  2.3470825e-04, -1.0666368e-04],\n",
       "        ...,\n",
       "        [-4.7090016e-03,  4.8154005e-04, -8.5429419e-05, ...,\n",
       "          1.9852400e-03,  9.4724499e-05, -1.3863838e-03],\n",
       "        [-4.6778172e-03,  2.2896686e-04, -2.2494490e-04, ...,\n",
       "          1.8075310e-03, -5.2484927e-05, -1.2516901e-03],\n",
       "        [-4.8316149e-03,  2.2431606e-04, -2.5518323e-04, ...,\n",
       "          1.4825589e-03,  2.1851227e-04, -9.6709625e-04]],\n",
       "\n",
       "       [[-1.0467601e-04, -9.1824593e-05,  4.2911695e-05, ...,\n",
       "         -4.1787236e-05,  2.0358998e-04, -9.4924151e-05],\n",
       "        [-1.6102733e-04,  1.1127415e-04,  2.1452190e-04, ...,\n",
       "         -4.5477649e-05,  3.4828339e-04, -1.4492602e-04],\n",
       "        [ 1.3425894e-04,  1.7224280e-04,  3.3418773e-04, ...,\n",
       "          3.1088328e-04,  1.4470679e-04, -7.1783048e-05],\n",
       "        ...,\n",
       "        [ 1.1744868e-04, -1.9927225e-03,  2.6266868e-03, ...,\n",
       "          9.1642718e-04, -6.5098621e-04,  2.8862033e-04],\n",
       "        [-2.0628214e-04, -1.5517246e-03,  2.5436124e-03, ...,\n",
       "          7.7363622e-04, -4.8542101e-04,  2.4383125e-04],\n",
       "        [-6.6362729e-04, -1.4580908e-03,  2.1766862e-03, ...,\n",
       "          4.2401676e-04, -2.4137253e-04,  3.7116403e-04]],\n",
       "\n",
       "       [[-1.0467601e-04, -9.1824593e-05,  4.2911695e-05, ...,\n",
       "         -4.1787236e-05,  2.0358998e-04, -9.4924151e-05],\n",
       "        [-7.7861419e-05, -2.2156224e-04,  2.5762158e-04, ...,\n",
       "          1.0802466e-04,  4.5581802e-04, -2.5503474e-04],\n",
       "        [ 3.6030851e-05, -4.8965466e-04,  5.7647965e-04, ...,\n",
       "          1.9642382e-04,  7.2763424e-04, -5.8230919e-05],\n",
       "        ...,\n",
       "        [-4.4272793e-04,  1.0070894e-03, -5.4416938e-05, ...,\n",
       "         -1.3923808e-04,  9.1691036e-04,  1.6630132e-03],\n",
       "        [-5.9997971e-04,  1.1492604e-03, -1.7435872e-04, ...,\n",
       "          5.2009906e-05,  1.0477804e-03,  1.6133579e-03],\n",
       "        [-7.5668498e-04,  8.5829163e-04, -2.4984821e-04, ...,\n",
       "          2.3316496e-04,  1.0340321e-03,  1.5105011e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.0467601e-04, -9.1824593e-05,  4.2911695e-05, ...,\n",
       "         -4.1787236e-05,  2.0358998e-04, -9.4924151e-05],\n",
       "        [-2.7731928e-04, -1.1927374e-04, -1.4294159e-04, ...,\n",
       "         -6.1512619e-05,  1.0008394e-03,  2.1682876e-04],\n",
       "        [-5.2985450e-04,  2.8387351e-05, -4.0357816e-04, ...,\n",
       "          1.8740988e-05,  1.4593783e-03,  7.3673012e-04],\n",
       "        ...,\n",
       "        [-2.4311659e-03,  3.0122057e-04, -1.2627809e-04, ...,\n",
       "          2.3928348e-03,  5.9515474e-05, -1.1302435e-03],\n",
       "        [-2.3534566e-03,  4.1074594e-04, -3.4459904e-04, ...,\n",
       "          2.7606008e-03, -4.9605867e-04, -1.5127350e-03],\n",
       "        [-2.3455895e-03,  5.6282309e-04, -4.4669829e-05, ...,\n",
       "          2.8499598e-03, -5.9889595e-04, -1.4202058e-03]],\n",
       "\n",
       "       [[-1.0467601e-04, -9.1824593e-05,  4.2911695e-05, ...,\n",
       "         -4.1787236e-05,  2.0358998e-04, -9.4924151e-05],\n",
       "        [-4.4018368e-04, -2.3716257e-05, -1.8354829e-05, ...,\n",
       "          1.4196646e-04,  4.5392482e-04, -4.8963138e-05],\n",
       "        [-7.0962188e-04,  1.8722699e-04, -2.1177962e-04, ...,\n",
       "          4.1408188e-04,  6.9018622e-04,  3.5035226e-04],\n",
       "        ...,\n",
       "        [-8.4916560e-04,  4.0590030e-04,  5.2599807e-04, ...,\n",
       "         -9.7853981e-04,  8.0500124e-04,  1.2921252e-03],\n",
       "        [-3.8413156e-04,  2.8977546e-04,  4.5361242e-04, ...,\n",
       "         -1.5800146e-03,  1.3976791e-03,  8.2531833e-04],\n",
       "        [ 6.6320325e-05,  1.1292588e-04,  3.2029339e-04, ...,\n",
       "         -2.0605810e-03,  1.9624920e-03,  3.5723907e-04]],\n",
       "\n",
       "       [[-1.0467601e-04, -9.1824593e-05,  4.2911695e-05, ...,\n",
       "         -4.1787236e-05,  2.0358998e-04, -9.4924151e-05],\n",
       "        [-5.0872547e-04, -2.5657288e-04,  2.7400855e-04, ...,\n",
       "         -3.0436530e-04,  4.9115962e-04, -2.0646183e-04],\n",
       "        [-9.4809919e-04, -2.8173876e-04,  3.0290810e-04, ...,\n",
       "         -2.6573153e-04,  7.6463615e-04, -2.5901161e-04],\n",
       "        ...,\n",
       "        [-2.3232544e-04, -2.3840721e-04,  6.1069950e-05, ...,\n",
       "         -4.5919130e-04,  4.1958559e-04,  1.0237768e-03],\n",
       "        [ 2.8606399e-05, -5.0857176e-05,  1.3830412e-04, ...,\n",
       "         -1.0468507e-03,  6.7664374e-04,  6.9538789e-04],\n",
       "        [ 3.6410624e-04,  1.9384226e-05,  1.6460010e-04, ...,\n",
       "         -1.6137527e-03,  1.0329986e-03,  3.4352075e-04]]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model을 bilud하기 위해 하나의 데이터를 넣는다\n",
    "for source_sample, target_sample in dataset.take(1): break\n",
    "model(source_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "accomplished-variance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  6144512   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  20979712  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  24590049  \n",
      "=================================================================\n",
      "Total params: 85,276,897\n",
      "Trainable params: 85,276,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() # 모델 구조 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "regular-professor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "610/610 [==============================] - 252s 413ms/step - loss: 3.1990 - accuracy: 0.5080 - val_loss: 2.7635 - val_accuracy: 0.5362\n",
      "Epoch 2/10\n",
      "610/610 [==============================] - 255s 418ms/step - loss: 2.6343 - accuracy: 0.5511 - val_loss: 2.3393 - val_accuracy: 0.5745\n",
      "Epoch 3/10\n",
      "610/610 [==============================] - 256s 419ms/step - loss: 2.2764 - accuracy: 0.5873 - val_loss: 1.9693 - val_accuracy: 0.6207\n",
      "Epoch 4/10\n",
      "610/610 [==============================] - 257s 421ms/step - loss: 1.9438 - accuracy: 0.6306 - val_loss: 1.6488 - val_accuracy: 0.6776\n",
      "Epoch 5/10\n",
      "610/610 [==============================] - 258s 423ms/step - loss: 1.6495 - accuracy: 0.6788 - val_loss: 1.3887 - val_accuracy: 0.7283\n",
      "Epoch 6/10\n",
      "610/610 [==============================] - 258s 423ms/step - loss: 1.4128 - accuracy: 0.7223 - val_loss: 1.2037 - val_accuracy: 0.7667\n",
      "Epoch 7/10\n",
      "610/610 [==============================] - 260s 427ms/step - loss: 1.2358 - accuracy: 0.7581 - val_loss: 1.0767 - val_accuracy: 0.7932\n",
      "Epoch 8/10\n",
      "610/610 [==============================] - 269s 441ms/step - loss: 1.1155 - accuracy: 0.7840 - val_loss: 1.0045 - val_accuracy: 0.8067\n",
      "Epoch 9/10\n",
      "610/610 [==============================] - 265s 434ms/step - loss: 1.0450 - accuracy: 0.7991 - val_loss: 0.9639 - val_accuracy: 0.8142\n",
      "Epoch 10/10\n",
      "610/610 [==============================] - 265s 434ms/step - loss: 1.0076 - accuracy: 0.8062 - val_loss: 0.9413 - val_accuracy: 0.8179\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss      = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = \"none\")\n",
    "\n",
    "model.compile(loss = loss, optimizer = optimizer, metrics = [\"accuracy\"])       # model을 compile하고\n",
    "history = model.fit(dataset, epochs = 10, validation_data = (enc_val, dec_val)) # 훈련시킨 후 그 결과를 history에 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "unexpected-acquisition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss 그래프 시각화\n",
    "train_accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy   = history.history[\"val_accuracy\"]\n",
    "train_loss     = history.history[\"loss\"]\n",
    "val_loss       = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(train_accuracy) + 1)\n",
    "\n",
    "plt.plot(epochs, train_loss, \"bo\", label = \"Training loss\")   # train_loss는 파란색 점 표시\n",
    "plt.plot(epochs, val_loss,   \"b\",  label = \"Validation loss\") # val loss는 파란색 실선 표시\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fiscal-bargain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence = \"<start>\", max_len = 20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input  = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype = tf.int64)\n",
    "    end_token   = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict      = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis = -1), axis = -1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis = 0)], axis = -1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "certified-quilt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i m not gonna crack <end> '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 테스트 : \"i love\"란 문장을 주고 20문장이 넘지 않는 문장을 만들게 하기\n",
    "generate_text(model, tokenizer, init_sentence = \"<start> i love\", max_len = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "upper-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회고록\n",
    "# 여기서 사용한 def 함수는 노드를 인용하여 만들었음 (보고 그대로 쓰고나, 일부 바꾸거나)\n",
    "# 노드를 참고하여 데이터를 가공하고 나누는데 train 데이터 갯수가 노드대로 안 나와서 조원과 상의하면서 어디가 잘못됬는지 한참 찾음.\n",
    "# 나중에 퍼실님께서 오셔서 신경쓸 필요 없다고 하여 넘어가도 되는걸 깨달음....\n",
    "# 노드에 나온것처럼 모델을 class로 만들고 학습을 시작함.\n",
    "# 그리고 프로젝트에서 준 단어(i love)를 모델에게 제시하고 문장을 만들라고 했더니 너무 쉬운 문장(\"I love you\")이 나와서 이게 맞는건가 싶음.\n",
    "# 먼가 이상해서 좀 살펴보니 데이터를 나누고 train만 쓰고 vaildation은 안씀.\n",
    "# 그래서 다시 코드를 고쳐서 학습시키고 실행함. 그랬더니 이번에는 \"i love you , i m not gonna crack\"라는 문장을 만들어 냄. (훈련시간 40분정도..)\n",
    "# 저번 과제에서 썻던 훈련과정 시각화 코드를 이용하여 봤더니 train, vaildation이 같이 잘 내려감.\n",
    "# 여기엔 없지만 embedding과 hidden값을 노드대로 하고, vaildation데이터를 쓴 모델을 학습시켜봤는데 (훈련시간 10분정도)\n",
    "# loss: 2.1354 - accuracy: 0.5980 - val_loss: 2.0499 - val_accuracy: 0.6080가 나오고 시각화는 수치만 다를뿐 모양은 비슷하게 나옴\n",
    "# 그 모델은 '<start> i love the way you lie <end> '라는 문장을 작문했음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-large",
   "metadata": {},
   "source": [
    "## 추가적으로 embedding, hidden을 다르게 줘서 돌린 결과    \n",
    "#### 각각 100분가량 훈련시간이 소요되었다.\n",
    "#### 코드는 같지만 작문결과가 다르다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-sodium",
   "metadata": {},
   "source": [
    "#### https://github.com/Soah-1994/Soah-1994.github.io/blob/repository/(E6)%206th%20project_v2.ipynb\n",
    "#### https://github.com/Soah-1994/Soah-1994.github.io/blob/repository/(E6)%206th%20project_v3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-guidance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
